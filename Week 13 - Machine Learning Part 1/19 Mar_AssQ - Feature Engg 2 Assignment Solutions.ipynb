{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b95b2113-43b5-41b5-91bf-d3cbc1060de6",
   "metadata": {},
   "source": [
    "Q1. What is Min-Max scaling, and how is it used in data preprocessing? Provide an example to illustrate its\n",
    "application."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8a3a7be-c8e2-4196-a633-267ad38c0820",
   "metadata": {},
   "source": [
    "Min-Max scaling, also known as normalization, is a data preprocessing technique used to scale the values of numerical features in a dataset to a specific range. The goal of Min-Max scaling is to transform the data so that it falls within a certain interval, typically between 0 and 1. This process is particularly useful when features have different ranges or units, and you want to ensure that they are on a similar scale.\n",
    "\n",
    "Formula:\\\n",
    "X_scaled = (X - X_min) / (X_max - X_min) \\\n",
    "Where:\\\n",
    "X_scaled is the scaled value of the feature. \\\n",
    "X is the original value of the feature. \\\n",
    "X_min is the minimum value of the feature in the dataset. \\\n",
    "X_max is the maximum value of the feature in the dataset.\n",
    "\n",
    "Example:\n",
    "\n",
    "Let's consider a simple example where we have a dataset with a feature representing ages. The original ages range from 18 to 65. We want to scale these ages to a range of [0, 1] using Min-Max scaling.\n",
    "\n",
    "Original ages: [18, 30, 45, 65]\n",
    "\n",
    "X_min (minimum age) = 18 \\\n",
    "X_max (maximum age) = 65\n",
    "\n",
    "Using the Min-Max scaling formula: \\\n",
    "Scaled age for 18: (18 - 18) / (65 - 18) = 0 \\\n",
    "Scaled age for 30: (30 - 18) / (65 - 18) ≈ 0.2558 \\\n",
    "Scaled age for 45: (45 - 18) / (65 - 18) ≈ 0.5789 \\\n",
    "Scaled age for 65: (65 - 18) / (65 - 18) = 1 \\\n",
    "So, after Min-Max scaling, the scaled ages would be approximately [0, 0.2558, 0.5789, 1]."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9956c03-97d0-4019-b09b-8a3d78dbc5d7",
   "metadata": {},
   "source": [
    "Q2. What is the Unit Vector technique in feature scaling, and how does it differ from Min-Max scaling?\n",
    "Provide an example to illustrate its application."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "117feeba-e3cf-41df-b59a-dbf1ec0a432c",
   "metadata": {},
   "source": [
    "The Unit Vector technique in feature scaling involves transforming the values of numerical features in a dataset so that each feature's magnitude (length) becomes 1 while preserving the original direction of the data points. This technique is also known as \"vector normalization\" or \"unit normalization.\" \\\n",
    "It's particularly useful when dealing with algorithms that are sensitive to the scale of features, such as distance-based algorithms like k-nearest neighbors (KNN) or when performing gradient descent optimization in machine learning models. Particularly useful when you want to focus on the direction of the data points rather than their magnitude. It's commonly used in cases where the direction or pattern of the data is more important than the absolute values of the features.\n",
    "\n",
    "Formula: \\\n",
    "Unit vector = Feature / ||Feature|| \\\n",
    "Where:\\\n",
    "Feature is the original value of the feature.\\\n",
    "||Feature|| is the magnitude (length) of the feature.\n",
    "\n",
    "Difference from Min-Max Scaling:\\\n",
    "Magnitude: Unit Vector technique scales each feature's magnitude to 1, while Min-Max scaling scales features to a specific range, often 0 to 1. \\\n",
    "Direction: Unit Vector technique only scales the magnitude, keeping the direction of the feature vector unchanged. Min-Max scaling retains the original direction as well. \\\n",
    "Applicability: Unit Vector technique is more suitable for cases where the direction of the feature matters, such as when dealing with vectors representing directions, angles, or when the algorithm you're using is sensitive to the feature's scale and magnitude.\n",
    "\n",
    "Example:\n",
    "\n",
    "Let's say you have a dataset with two features, \"Age\" and \"Income\". You want to normalize these features using both techniques.\n",
    "\n",
    "Original data: \\\n",
    "Age: [25, 40, 30] \\\n",
    "Income: [50000, 75000, 60000]\n",
    "\n",
    "Unit Vector technique: \\\n",
    "Normalized Age: [0.4472, 0.8944, 0.4472] \\\n",
    "Normalized Income: [0.5774, 0.8660, 0.5774]\n",
    "\n",
    "Min-Max Scaling (assuming income range of 0-100000): \\\n",
    "Scaled Age: [0.25, 0.75, 0.5] \\\n",
    "Scaled Income: [0.25, 0.75, 0.5]\n",
    "\n",
    "In this example, Unit Vector technique normalizes the magnitudes of the features, while Min-Max scaling brings the features into the specified range."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db609a5c-e8f2-4935-a7dc-5d4dd1e5f5bd",
   "metadata": {},
   "source": [
    "Q3. What is PCA (Principle Component Analysis), and how is it used in dimensionality reduction? Provide an\n",
    "example to illustrate its application."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2be9f047-b345-47c8-aebe-41f7abc69f8b",
   "metadata": {},
   "source": [
    "Principal Component Analysis (PCA) is a dimensionality reduction technique used to transform high-dimensional data into a lower-dimensional space while retaining as much of the original variability as possible. It achieves this by transforming the data into a new coordinate system, where the new axes (principal components) are orthogonal to each other and aligned with the directions of maximum variance in the original data.\n",
    "\n",
    "Here's a simple step-by-step explanation of how PCA works and how it's used for dimensionality reduction:\n",
    "\n",
    "1. Data Preparation: Let's say you have a dataset with multiple features (dimensions) like height, weight, age, etc.\n",
    "\n",
    "2. Mean Centering: The first step is to subtract the mean from each feature, ensuring that the centered data has a mean of zero.\n",
    "\n",
    "3. Covariance Matrix: Calculate the covariance matrix of the centered data. The covariance matrix shows the relationships between different features and indicates how they vary together.\n",
    "\n",
    "4. Eigenvalue Decomposition: Perform eigenvalue decomposition on the covariance matrix. This yields eigenvalues and eigenvectors. Eigenvectors are the new axes, and eigenvalues represent the amount of variance captured by each eigenvector.\n",
    "\n",
    "5. Select Principal Components: Sort the eigenvectors by their corresponding eigenvalues in descending order. Choose the top k eigenvectors (principal components) based on the amount of variance they explain. These are the dimensions in the new coordinate system.\n",
    "\n",
    "6. Projection: Project the original data onto the selected principal components. This generates a lower-dimensional representation of the data.\n",
    "\n",
    "Here's an example to illustrate the application of PCA in dimensionality reduction:\n",
    "\n",
    "Suppose you have a dataset with three variables: height, weight, and age, and you want to reduce these three variables to two principal components for visualization purposes.\\\n",
    "Original Data (3D):\\\n",
    "Height\n",
    "Weight\n",
    "Age\n",
    "\n",
    "Using the above 6 steps:\n",
    "The resulting data will now have only two dimensions, represented by the two principal components. These components will be orthogonal and capture the most significant variability in the original data. This reduced representation can be used for visualization, analysis, or as input for further machine learning tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c20f04d-82e7-44b4-b5fd-742ba1036f0b",
   "metadata": {},
   "source": [
    "Q4. What is the relationship between PCA and Feature Extraction, and how can PCA be used for Feature\n",
    "Extraction? Provide an example to illustrate this concept."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "057c7858-c4ec-4c18-bb99-855bbcfc980f",
   "metadata": {},
   "source": [
    "Relationship between PCA and Feature Extraction:\\\n",
    "Feature extraction involves transforming the original features of a dataset into a new set of features that capture the most important information while reducing noise and redundancy.\\\n",
    "PCA achieves this by creating new features, known as principal components, that are linear combinations of the original features and capture the most significant variations in the data. When using PCA for feature extraction, you can select a subset of the principal components that collectively capture a desired percentage of the total variance in the data. This allows you to retain the most informative aspects of the original data while reducing its dimensionality.\n",
    "\n",
    "Example:\\\n",
    "Let's say you have a dataset of images, each represented as a vector of pixel values. Each image has 1000 pixels, resulting in a high-dimensional feature space. You want to extract meaningful features that capture the main variations in these images.\n",
    "\n",
    "Original Data: Each image is represented by a 1000-dimensional vector of pixel values.\n",
    "\n",
    "Applying PCA for Feature Extraction:\n",
    "1. Compute the covariance matrix of the dataset.\n",
    "2. Calculate the eigenvectors and eigenvalues of the covariance matrix.\n",
    "3. Sort the eigenvectors based on their corresponding eigenvalues in decreasing order.\n",
    "4. Select the top 'k' eigenvectors (principal components) that explain most of the variance (where 'k' is the desired reduced dimensionality).\n",
    "5. Projecting Data onto Reduced-Dimensional Space: Using the selected 'k' eigenvectors as a transformation matrix, you can project each original image onto the lower-dimensional space spanned by these eigenvectors. This effectively extracts a set of 'k' features for each image.\n",
    "\n",
    "By reducing the dimensionality of the data using PCA, you've achieved feature extraction. The new features (principal components) are combinations of the original pixel values, emphasizing the most important information while reducing noise."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbbf9b80-0971-4fff-9b16-d3f59290eb98",
   "metadata": {},
   "source": [
    "Q5. You are working on a project to build a recommendation system for a food delivery service. The dataset\n",
    "contains features such as price, rating, and delivery time. Explain how you would use Min-Max scaling to\n",
    "preprocess the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa5a1785-abd8-4369-852b-cd11947b2812",
   "metadata": {},
   "source": [
    "Min-Max scaling is particularly useful when the features have different scales and ranges. By applying Min-Max scaling, we ensure that all the features are treated equally and contribute effectively to recommendation system, enhancing the system's performance and reliability.\n",
    "\n",
    "Here's how we can use Min-Max scaling:\n",
    "\n",
    "1. Understand the Features: \\\n",
    "Begin by understanding the characteristics of the features we're dealing with: \\\n",
    "Price: The cost of the food items. Typically, prices can vary significantly, and the range of values could be substantial.\\\n",
    "Rating: Customer ratings, often on a scale of 1 to 5. Ratings are bounded within a range and may not need extensive scaling.\\\n",
    "Delivery Time: The time taken for the food to be delivered. This could be in minutes, hours, etc. \n",
    "\n",
    "2. Identify the Scaling Range: \\\n",
    "Decide on the desired scaling range. Commonly, Min-Max scaling transforms features to a range between 0 and 1. However, we can choose a different range if it's more appropriate for the data and task.\n",
    "\n",
    "3. Apply Min-Max Scaling: \\\n",
    "For each feature (price, rating, delivery time), calculate the minimum and maximum values present in the dataset.\n",
    "Then, for each individual data point, apply the Min-Max scaling formula to transform the feature value to the desired range: \\\n",
    "scaled_value = (original_value - min_value) / (max_value - min_value)\\\n",
    "Here, original_value is the value of the feature for a particular data point, and min_value and max_value are the minimum and maximum values for that feature in the entire dataset, respectively. The result, scaled_value, will now fall within the specified scaling range (e.g., 0 to 1).\n",
    "\n",
    "4. Interpretation: \\\n",
    "After scaling, the features will now be in a consistent range, making it easier for machine learning algorithms to work with them. The scaled values retain the relative relationships between data points, preserving the patterns in the data.\n",
    "\n",
    "5. Use in Recommendation System: \\\n",
    "Once we have scaled the features, we can use them as inputs to our recommendation system. Machine learning algorithms, such as collaborative filtering or matrix factorization, can then utilize the scaled features to generate recommendations that are more accurate and meaningful."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1af4764d-9aa6-4d24-8fdb-c7741b05a226",
   "metadata": {},
   "source": [
    "Q6. You are working on a project to build a model to predict stock prices. The dataset contains many\n",
    "features, such as company financial data and market trends. Explain how you would use PCA to reduce the\n",
    "dimensionality of the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8286cf7-7590-448d-af6e-334752b99ec8",
   "metadata": {},
   "source": [
    "Using Principal Component Analysis (PCA) to reduce the dimensionality of a dataset when building a model to predict stock prices can help address the curse of dimensionality, improve model performance, and enhance interpretability. \n",
    "\n",
    "Here's how you can apply PCA to achieve dimensionality reduction in the context of predicting stock prices: \n",
    "1. Understand the Dataset: \\\n",
    "First, you need to have a good understanding of the features in your dataset, which include company financial data and market trends. These features might be correlated with each other, and some could potentially contain noise or redundant information.\n",
    "\n",
    "2. Standardize the Data: \\\n",
    "Before applying PCA, it's important to standardize the data by subtracting the mean and scaling each feature to have unit variance. This step ensures that features with larger scales don't dominate the PCA process.\n",
    "\n",
    "3. Compute Covariance Matrix: \\\n",
    "PCA works by finding the principal components that capture the most variance in the data. To do this, you compute the covariance matrix of the standardized data. The covariance matrix shows how much each feature varies with respect to others.\n",
    "\n",
    "4. Calculate Eigenvectors and Eigenvalues: \\\n",
    "The next step involves calculating the eigenvectors and eigenvalues of the covariance matrix. Eigenvectors represent the directions in the original feature space along which the data varies the most, and eigenvalues represent the amount of variance along those directions.\n",
    "\n",
    "5. Sort Eigenvectors by Eigenvalues: \\\n",
    "Sort the eigenvectors in decreasing order of their corresponding eigenvalues. The eigenvectors with the highest eigenvalues capture the most variance in the data.\n",
    "\n",
    "6. Choose Principal Components: \\\n",
    "To reduce dimensionality, you can choose the top-k eigenvectors (principal components) based on the amount of variance they explain. Typically, you'd choose enough principal components to retain a certain percentage of the total variance (e.g., 95% or 99%).\n",
    "\n",
    "7. Projection: \\\n",
    "The final step involves projecting your original data onto the selected principal components. This essentially transforms your data into a new lower-dimensional space.\n",
    "\n",
    "By performing PCA and reducing the dimensionality of your dataset, we'll have a more compact representation of the data that retains most of the important information. This can lead to improved model performance, reduced computational complexity, and potentially more stable predictions in your stock price prediction project."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a971a854-9588-43ef-b04e-3d7a33e7bda5",
   "metadata": {},
   "source": [
    "Q7. For a dataset containing the following values: [1, 5, 10, 15, 20], perform Min-Max scaling to transform the\n",
    "values to a range of -1 to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5c1092ba-9e8a-4230-9183-a85cae3b276f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original dataset: [1, 5, 10, 15, 20]\n",
      "Scaled values: [-0.9999999999999999, -0.5789473684210525, -0.05263157894736836, 0.47368421052631593, 1.0]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Original dataset\n",
    "data = [1, 5, 10, 15, 20]\n",
    "\n",
    "# Create a MinMaxScaler instance\n",
    "scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "\n",
    "# Fit and transform the data using the scaler\n",
    "scaled_data = scaler.fit_transform([[x] for x in data])\n",
    "\n",
    "# Extract the scaled values from the scaled_data array\n",
    "scaled_values = [scaled[0] for scaled in scaled_data]\n",
    "\n",
    "print(\"Original dataset:\", data)\n",
    "print(\"Scaled values:\", scaled_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01d570ea-9a02-4a68-8e54-e6be05277fe6",
   "metadata": {},
   "source": [
    "Q8. For a dataset containing the following features: [height, weight, age, gender, blood pressure], perform\n",
    "Feature Extraction using PCA. How many principal components would you choose to retain, and why?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af22da82-2fe5-4b08-b310-e7cc323010fc",
   "metadata": {},
   "source": [
    "The goal is to retain a sufficient number of principal components that capture most of the variability in the data while reducing the dimensionality of the dataset. \n",
    "\n",
    "Here are the general steps to decide how many principal components to retain:\n",
    "\n",
    "1. Calculate the Covariance Matrix: \n",
    "Calculate the covariance matrix of the original feature matrix.\n",
    "\n",
    "2. Calculate Eigenvectors and Eigenvalues: \n",
    "Compute the eigenvectors and eigenvalues of the covariance matrix. The eigenvectors represent the directions of maximum variance in the data, and the eigenvalues represent the amount of variance along each eigenvector.\n",
    "\n",
    "3. Sort Eigenvalues: \n",
    "Sort the eigenvalues in descending order. The eigenvalues indicate the amount of variance explained by each corresponding eigenvector.\n",
    "\n",
    "4. Explained Variance: \n",
    "Calculate the explained variance ratio for each eigenvalue by dividing it by the sum of all eigenvalues. This ratio indicates the proportion of the total variance that each principal component explains.\n",
    "\n",
    "5. Cumulative Explained Variance: \n",
    "Calculate the cumulative explained variance by summing up the explained variance ratios from step 4.\n",
    "\n",
    "6. Choose Number of Components: \n",
    "Determine how many principal components to retain by looking at the cumulative explained variance plot. A common approach is to choose the smallest number of components that explains a high percentage (e.g., 95% or 99%) of the total variance.\n",
    "\n",
    "In this case, where the dataset contains the features [height, weight, age, gender, blood pressure], the number of principal components to retain would depend on the specific characteristics of the data. We would perform the steps mentioned above and then decide based on the cumulative explained variance plot. The general principle is to retain as few components as possible while retaining a high percentage of the total variance.\n",
    "\n",
    "For instance, if after performing PCA we find that the first two or three principal components explain a significant portion of the total variance (e.g., 90% or more), you might choose to retain those components. However, the exact number of components to retain is a decision that should take into consideration our specific goals and the trade-off between reducing dimensionality and preserving meaningful information in our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c19d5bf-69ce-44d9-99a5-36b3e42ab8c9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
